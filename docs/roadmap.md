# Roadmap â€” mlx-jaccl-cluster

> Living document. Last updated: 2025-07-14
>
> This document sketches what we have, what we're missing relative to exo,
> and the phased plan to close the gap â€” dashboard v2, tool support,
> model management, observability, and beyond.

---

## Table of Contents

- [1. Current State](#1-current-state)
- [2. Gap Analysis vs exo](#2-gap-analysis-vs-exo)
- [3. Priority Matrix](#3-priority-matrix)
- [4. Phase 1 â€” Observability & Monitoring](#4-phase-1--observability--monitoring)
- [5. Phase 2 â€” Dashboard v2](#5-phase-2--dashboard-v2)
- [6. Phase 3 â€” Model Management](#6-phase-3--model-management)
- [7. Phase 4 â€” Tool Support & Structured Output](#7-phase-4--tool-support--structured-output)
- [8. Phase 5 â€” API Parity & Ecosystem](#8-phase-5--api-parity--ecosystem)
- [9. Phase 6 â€” Advanced Inference](#9-phase-6--advanced-inference)
- [10. Non-Goals](#10-non-goals)
- [11. Architecture Decisions](#11-architecture-decisions)
- [12. Dashboard v2 â€” Wireframe](#12-dashboard-v2--wireframe)
- [13. Implementation Notes](#13-implementation-notes)
- [14. Dependency Budget](#14-dependency-budget)
- [15. Success Criteria](#15-success-criteria)

---

## 1. Current State

### What Works Today (v0.1)

| Area | Status | Details |
|---|---|---|
| RDMA transport | âœ… Production-ready | 8.05 GB/s peak, 25.5 Âµs latency, stress-tested |
| Tensor-parallel inference | âœ… Working | `mlx_lm.sharded_load` across 2 nodes |
| OpenAI-compatible API | âœ… Working | `/v1/chat/completions`, `/v1/completions`, streaming SSE |
| Dashboard | âœ… Basic | HTMX+SSE, tok/s sparkline, queue depth, chat UI |
| Cluster tooling | âœ… Solid | Makefile, setup, bootstrap, verify, sync, benchmarks |
| RDMA test suite | âœ… Comprehensive | Correctness, latency, bandwidth, stress modes |
| Documentation | âœ… Good | Quickstart, from-scratch, comparison, scripts reference |

### What's Missing (Honest Assessment)

| Area | Status | Impact |
|---|---|---|
| Live RAM / memory monitoring | âŒ | Can't see if we're approaching OOM during inference |
| Live RDMA link health | âŒ | Static label only â€” no real-time bandwidth probe |
| Model download & management | âŒ | Manual `huggingface-cli` + `rsync` every time |
| Tool calls / function calling | âŒ | Can't use with agents, LangChain, OpenAI SDK tools |
| Structured output / JSON mode | âŒ | No `response_format` support |
| KV prefix cache | âŒ | Every request re-processes the full prompt |
| Ollama API compatibility | âŒ | Can't use with Ollama-native clients |
| Multi-model serving | âŒ | One model per server instance |
| Request logging / tracing | âŒ | No persistent logs, no request history |
| Prometheus / Grafana export | âŒ | No standard metrics format |
| Node failure detection | âŒ | Worker disconnect = silent hang |
| Image generation | âŒ | No Flux / image pipeline |

---

## 2. Gap Analysis vs exo

Detailed feature-by-feature comparison showing what exo has, what we have, and whether closing the gap makes sense for our use case.

### Server / API Layer

| Feature | exo | Us (v0.1) | Gap | Priority |
|---|---|---|---|---|
| `/v1/chat/completions` | âœ… | âœ… | â€” | â€” |
| `/v1/completions` | âœ… | âœ… | â€” | â€” |
| SSE streaming | âœ… | âœ… | â€” | â€” |
| Tool calls / function calling | âœ… | âŒ | **Big** | ðŸ”´ High |
| Structured output (`response_format`) | âœ… | âŒ | **Big** | ðŸ”´ High |
| `temperature`, `top_p`, `top_k` | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| `stop` sequences | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| `n` (multiple completions) | âœ… | âŒ | Small | ðŸŸ¢ Low |
| `logprobs` | âœ… | âŒ | Small | ðŸŸ¢ Low |
| Token usage in streaming | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| Ollama `/api/generate` | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| Ollama `/api/chat` | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| `/v1/embeddings` | âŒ | âŒ | â€” | Future |

### Dashboard / Observability

| Feature | exo | Us (v0.1) | Gap | Priority |
|---|---|---|---|---|
| Live tok/s + sparkline | âœ… | âœ… | â€” | â€” |
| Cluster topology table | âœ… | âœ… | â€” | â€” |
| Chat UI (streaming) | âœ… | âœ… | â€” | â€” |
| Queue depth indicator | âœ… via inference | âœ… | â€” | â€” |
| **RAM / unified memory usage** | âŒ | âŒ | **Both miss** | ðŸ”´ High |
| **Live RDMA bandwidth probe** | âŒ | âŒ static | **We should own this** | ðŸ”´ High |
| **Per-node GPU memory** | âŒ | âŒ | **Both miss** | ðŸ”´ High |
| D3.js topology graph | âœ… animated | âŒ | Medium | ðŸŸ¡ Medium |
| Model download progress | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| Token heatmap / attention | âœ… | âŒ | Small | ðŸŸ¢ Low |
| Generation traces | âœ… | âŒ | Small | ðŸŸ¢ Low |
| Error log viewer | âŒ | âŒ | Medium | ðŸŸ¡ Medium |
| Request history table | âŒ | âŒ | Medium | ðŸŸ¡ Medium |
| Prometheus `/metrics` | âŒ | âŒ | Medium | ðŸŸ¡ Medium |

### Model Management

| Feature | exo | Us (v0.1) | Gap | Priority |
|---|---|---|---|---|
| Built-in model download | âœ… coordinator | âŒ manual CLI | **Big** | ðŸ”´ High |
| Download progress tracking | âœ… per-shard | âŒ | **Big** | ðŸ”´ High |
| Auto-sync to all nodes | âœ… via tasks | âŒ manual rsync | Medium | ðŸŸ¡ Medium |
| Model registry / list | âœ… in code | âŒ | Medium | ðŸŸ¡ Medium |
| Hot model swap | âŒ | âŒ | â€” | Future |

### Infrastructure / Operations

| Feature | exo | Us (v0.1) | Gap | Priority |
|---|---|---|---|---|
| Auto-discovery (libp2p) | âœ… | âŒ explicit | **Not a gap** | â€” |
| Leader election | âœ… | âŒ | Not needed (2 nodes) | â€” |
| Node health monitoring | Partial | âŒ | Medium | ðŸŸ¡ Medium |
| Worker disconnect detection | âœ… | âŒ | Medium | ðŸŸ¡ Medium |
| Graceful shutdown | Partial | âŒ | Medium | ðŸŸ¡ Medium |
| Server logs (persistent) | âŒ | âŒ | Medium | ðŸŸ¡ Medium |
| CI / automated tests | Partial | âŒ | Medium | ðŸŸ¡ Medium |

### Things We Do Better (Keep / Protect)

| Advantage | Details |
|---|---|
| **RDMA actually works** | 8.05 GB/s proven; exo's auto-mapping is broken on M4 Pro |
| **Deterministic startup** | No race conditions, no election timeouts |
| **Zero build toolchain** | No Rust, no Node.js, no npm, no Swift |
| **Debuggable** | ~2K lines; any failure is traceable in minutes |
| **Stock MLX** | Uses official `mlx` from PyPI â€” no custom forks |
| **3-minute setup** | `make setup` â†’ `make rdma-test` â†’ `make server` |
| **Explicit configuration** | Hostfile gives full control; no magic |

---

## 3. Priority Matrix

Quadrant view â€” **Impact** (value to daily use) vs **Effort** (implementation complexity).

```
                          HIGH IMPACT
                              â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                  â”‚                  â”‚
           â”‚  RAM monitoring  â”‚  Tool calls      â”‚
           â”‚  RDMA live probe â”‚  Structured out   â”‚
           â”‚  Model download  â”‚  Ollama compat    â”‚
           â”‚  Worker health   â”‚  KV prefix cache  â”‚
           â”‚                  â”‚                  â”‚
LOW EFFORT â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ HIGH EFFORT
           â”‚                  â”‚                  â”‚
           â”‚  Sampling params â”‚  D3 topology      â”‚
           â”‚  Stop sequences  â”‚  Token heatmap    â”‚
           â”‚  Error log view  â”‚  Image generation  â”‚
           â”‚  Request history â”‚  Pipeline parallel â”‚
           â”‚  Prometheus      â”‚  Multi-model       â”‚
           â”‚                  â”‚                  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                          LOW IMPACT
```

**Do first** (top-left): High impact, low effort â€” observability, model management
**Do next** (top-right): High impact, high effort â€” tool calls, Ollama, KV cache
**Do later** (bottom-left): Low impact, low effort â€” sampling params, logging
**Probably never** (bottom-right): Low impact, high effort â€” image gen, pipeline parallel

---

## 4. Phase 1 â€” Observability & Monitoring

> **Goal:** See everything happening in the cluster in real time.
> **Effort:** ~2â€“3 days. **Impact:** Transforms daily operations.

### 4.1 Live RAM / Unified Memory Monitoring

**Problem:** We have 48 GB unified memory per node but no visibility into usage during inference. A large model + long context can silently approach OOM and crash.

**Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Memory (rank 0 â€” mac.home)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  37.4 / 48 GBâ”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  78% used     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  Model: 14.2 GB â”‚ KV cache: 2.1 GB â”‚ OS: 21.1 GB       â”‚
â”‚                                                         â”‚
â”‚  Memory (rank 1 â€” mac2)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  36.8 / 48 GBâ”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  77% used     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data source â€” per-node memory probe:**

```python
# Runs on every rank, reports to rank 0 via control-plane
import mlx.core as mx
import resource

def memory_snapshot() -> dict:
    info = mx.device_info()
    return {
        "total_gb": round(info["memory_size"] / (1024**3), 1),
        "working_set_gb": round(info["max_recommended_working_set_size"] / (1024**3), 1),
        "cache_gb": round(mx.metal.get_cache_memory() / (1024**3), 2),
        "active_gb": round(mx.metal.get_active_memory() / (1024**3), 2),
        "peak_gb": round(mx.metal.get_peak_memory() / (1024**3), 2),
        "rss_gb": round(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1024**3), 2),
    }
```

**Transport:** Workers periodically send memory snapshots to rank 0 over the existing TCP control-plane (new message type: `{"type": "metrics", "memory": {...}}`). No new connections needed.

**Dashboard integration:** New panel in dashboard, updated via SSE every 2 seconds. Color-coded bars (green < 70%, yellow 70â€“85%, red > 85%).

### 4.2 Live RDMA Link Health Probe

**Problem:** The dashboard shows a static "~8 GB/s" label. We need to know if the link is degraded or down.

**Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RDMA Link                                              â”‚
â”‚  rdma_en4 â†â†’ rdma_en4                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  7.92 GB/s          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  Latency: 26.1 Âµs â”‚ Status: â— ACTIVE â”‚ Last check: 3s  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation approach â€” lightweight background probe:**

- A background thread on rank 0 performs a small `all_sum` (e.g., 4 KB) every 10 seconds
- Measures round-trip latency
- Every 60 seconds, performs a larger probe (4 MB) to estimate bandwidth
- Reports results to the dashboard via the existing `MetricsStore`
- Does NOT interfere with inference (runs only when queue is empty)

**Key constraint:** The probe must NOT run during active generation. Use a lock shared with the `_queue_worker` to ensure mutual exclusion.

```python
# Pseudocode for RDMA health probe
class RDMAProbe:
    def __init__(self, world):
        self.world = world
        self.last_bw_gbps = 0.0
        self.last_latency_us = 0.0
        self.link_active = True
        self._generation_lock = asyncio.Lock()  # shared with queue_worker

    async def probe_latency(self):
        """4-byte all_sum â€” measures pure round-trip."""
        async with self._generation_lock:
            x = mx.ones(1)
            t0 = time.perf_counter()
            mx.distributed.all_sum(x)
            mx.eval(x)
            self.last_latency_us = (time.perf_counter() - t0) * 1e6

    async def probe_bandwidth(self):
        """4 MB all_sum â€” estimates sustained bandwidth."""
        async with self._generation_lock:
            x = mx.ones(1_048_576)  # 4 MB float32
            t0 = time.perf_counter()
            mx.distributed.all_sum(x)
            mx.eval(x)
            elapsed = time.perf_counter() - t0
            self.last_bw_gbps = (4.0 / 1024) / elapsed  # GB/s
            del x
            mx.clear_cache()
```

### 4.3 Worker Health & Disconnect Detection

**Problem:** If a worker process dies or the TB cable is unplugged, rank 0 hangs forever in `rank0_wait_done()`.

**Design:**

- TCP control-plane sockets get a heartbeat: workers send `{"type": "heartbeat"}` every 5 seconds
- Rank 0 tracks last heartbeat time per worker
- If no heartbeat for 15 seconds â†’ mark worker as `DISCONNECTED`
- Dashboard shows per-node status: `ACTIVE` / `DEGRADED` / `DISCONNECTED`
- On disconnect, queued requests get a 503 error instead of hanging forever

### 4.4 Request History & Error Log

**Problem:** No way to see past requests, errors, or debug failed generations.

**Design:**

- Ring buffer of last 200 requests with: timestamp, kind, prompt (truncated), tokens, latency, status
- Ring buffer of last 50 errors with: timestamp, error type, message, traceback
- Exposed via:
  - `GET /requests` â†’ JSON array of recent requests
  - `GET /errors` â†’ JSON array of recent errors
  - Dashboard panel with scrollable table

### 4.5 Prometheus Metrics Export

**Design:**

```
GET /metrics

# HELP mlx_cluster_requests_total Total inference requests
# TYPE mlx_cluster_requests_total counter
mlx_cluster_requests_total 1423

# HELP mlx_cluster_tokens_generated_total Total tokens generated
# TYPE mlx_cluster_tokens_generated_total counter
mlx_cluster_tokens_generated_total 182947

# HELP mlx_cluster_tokens_per_second Current tokens per second
# TYPE mlx_cluster_tokens_per_second gauge
mlx_cluster_tokens_per_second 62.3

# HELP mlx_cluster_queue_depth Current queue depth
# TYPE mlx_cluster_queue_depth gauge
mlx_cluster_queue_depth 2

# HELP mlx_cluster_memory_used_bytes Unified memory used per rank
# TYPE mlx_cluster_memory_used_bytes gauge
mlx_cluster_memory_used_bytes{rank="0"} 40265318400
mlx_cluster_memory_used_bytes{rank="1"} 39528046592

# HELP mlx_cluster_rdma_bandwidth_gbps Last measured RDMA bandwidth
# TYPE mlx_cluster_rdma_bandwidth_gbps gauge
mlx_cluster_rdma_bandwidth_gbps 7.92

# HELP mlx_cluster_rdma_latency_us Last measured RDMA latency
# TYPE mlx_cluster_rdma_latency_us gauge
mlx_cluster_rdma_latency_us 25.8
```

**No new dependency.** Plain text Prometheus exposition format is trivial to generate.

### 4.6 New Makefile Targets

```makefile
make status          # Full cluster status: nodes, memory, RDMA, queue, model
make logs            # Tail server logs (rank 0)
make monitor         # Watch mode: refresh status every 5s
make download MODEL=mlx-community/Qwen3-4B  # Download + sync model (Phase 3)
```

---

## 5. Phase 2 â€” Dashboard v2

> **Goal:** A dashboard that rivals exo's SvelteKit UI â€” but still zero build step.
> **Effort:** ~3â€“4 days. **Impact:** Professional-grade monitoring.

### 5.1 New Panels

The dashboard v2 adds these panels to the existing layout:

| Panel | Data Source | Update Frequency |
|---|---|---|
| **Memory gauges** (per node) | Worker heartbeats via control-plane | Every 2s |
| **RDMA link monitor** | Background probe | Every 10s (latency), 60s (bandwidth) |
| **Node health grid** | Worker heartbeats | Every 5s |
| **Request history table** | Ring buffer | On each request |
| **Error log** | Ring buffer | On error |
| **Model info card** | Static at startup | Once |
| **D3-lite topology** | Hostfile + RDMA probe | Every 10s |

### 5.2 Dashboard Layout (Wireframe)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš¡ mlx-jaccl-cluster  â”‚  Qwen3-4B-Instruct  â”‚  â— Online  â”‚  /docs   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€ Cluster Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€ RDMA Link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                               â”‚  â”‚                                â”‚ â”‚
â”‚  â”‚  Nodes: 2/2 online            â”‚  â”‚  â— ACTIVE â€” rdma_en4 â†” rdma_en4â”‚
â”‚  â”‚  Model: Qwen3-4B (4-bit)      â”‚  â”‚  Bandwidth: 7.92 GB/s          â”‚ â”‚
â”‚  â”‚  World size: 2                â”‚  â”‚  Latency:   26.1 Âµs            â”‚ â”‚
â”‚  â”‚  Uptime: 2h 14m               â”‚  â”‚  Last probe: 3s ago            â”‚ â”‚
â”‚  â”‚  Total requests: 1,423        â”‚  â”‚                                â”‚ â”‚
â”‚  â”‚  Total tokens: 182,947        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚                               â”‚  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚                                     â”‚  99% of theoretical max        â”‚ â”‚
â”‚  â”Œâ”€â”€â”€ Memory (rank 0) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                                     â”‚
â”‚  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â”‚    â”‚  â”Œâ”€â”€â”€ Performance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚                                 â”‚ â”‚
â”‚  â”‚  37.4 / 48 GB  (78%)        â”‚  â”‚  Avg tok/s (60s): 62.3          â”‚ â”‚
â”‚  â”‚  Active: 14.2 â”‚ Cache: 2.1  â”‚  â”‚  Peak tok/s:      71.8          â”‚ â”‚
â”‚  â”‚  Peak: 16.3 GB              â”‚  â”‚  Avg latency:     4.12s         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Queue: 1/8  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘           â”‚ â”‚
â”‚                                     â”‚                                 â”‚ â”‚
â”‚  â”Œâ”€â”€â”€ Memory (rank 1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€ tok/s sparkline â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚  â”‚  â•±â•²   â•±â•²  â•±â•²             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚    â”‚  â”‚  â”‚ â•±  â•²_â•±  â•²â•±  â•²_â•±â•²         â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚  36.8 / 48 GB  (77%)        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚  Active: 14.0 â”‚ Cache: 2.0  â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€ Topology â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          rdma_en4          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚   â”‚  rank 0  â”‚  â—„â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º   â”‚  rank 1  â”‚           â”‚  â”‚
â”‚  â”‚   â”‚  mac.homeâ”‚        8.05 GB/s           â”‚  mac2    â”‚           â”‚  â”‚
â”‚  â”‚   â”‚  coord   â”‚        25.5 Âµs             â”‚  worker  â”‚           â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€ Request History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Time       â”‚ Kind â”‚ Tokens â”‚ Latency â”‚ tok/s â”‚ Status           â”‚  â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚
â”‚  â”‚  14:23:01   â”‚ chat â”‚ 128    â”‚ 2.06s   â”‚ 62.1  â”‚ âœ… ok            â”‚  â”‚
â”‚  â”‚  14:22:45   â”‚ chat â”‚ 256    â”‚ 4.12s   â”‚ 62.1  â”‚ âœ… ok            â”‚  â”‚
â”‚  â”‚  14:22:30   â”‚ cmpl â”‚  64    â”‚ 1.03s   â”‚ 62.1  â”‚ âœ… ok            â”‚  â”‚
â”‚  â”‚  14:21:58   â”‚ chat â”‚  32    â”‚ 0.52s   â”‚ 61.5  â”‚ âœ… ok            â”‚  â”‚
â”‚  â”‚  14:21:12   â”‚ chat â”‚ 512    â”‚ 8.31s   â”‚ 61.6  â”‚ âš ï¸ slow          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€ Chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  (existing chat UI â€” keep as-is)                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 D3-lite Topology (No D3.js Dependency)

Instead of importing D3.js (which exo does), we draw the topology with **pure SVG** updated via HTMX/SSE:

- Nodes rendered as rounded rectangles
- RDMA links as animated dashed lines (CSS animation, no JS library)
- Link color: green = healthy, yellow = degraded, red = down
- Bandwidth label on the link, updated from the RDMA probe
- Works with 2-node and 4-node topologies (reads from hostfile)

### 5.4 SSE Event Schema (v2)

Current SSE pushes a flat JSON blob. v2 adds structured sections:

```json
{
  "uptime_s": 8040,
  "total_requests": 1423,
  "total_tokens": 182947,
  "avg_tps_60s": 62.3,
  "peak_tps_60s": 71.8,
  "avg_latency_60s": 4.12,
  "queue_size": 1,
  "queue_max": 8,
  "history": [ ... ],

  "memory": {
    "0": { "active_gb": 14.2, "cache_gb": 2.1, "peak_gb": 16.3, "total_gb": 48.0 },
    "1": { "active_gb": 14.0, "cache_gb": 2.0, "peak_gb": 16.1, "total_gb": 48.0 }
  },
  "rdma": {
    "bandwidth_gbps": 7.92,
    "latency_us": 26.1,
    "link_active": true,
    "last_probe_s": 3
  },
  "nodes": {
    "0": { "status": "active", "hostname": "mac.home", "last_heartbeat_s": 0 },
    "1": { "status": "active", "hostname": "mac2", "last_heartbeat_s": 2 }
  }
}
```

---

## 6. Phase 3 â€” Model Management

> **Goal:** Download, sync, and manage models without leaving the terminal (or the dashboard).
> **Effort:** ~2â€“3 days. **Impact:** Eliminates the most tedious manual step.

### 6.1 `make download` Target

```bash
# Download a model from HuggingFace and sync to all nodes
make download MODEL=mlx-community/Qwen3-4B-Instruct-2507-4bit

# Download to a custom directory
make download MODEL=mlx-community/Qwen3-4B-Instruct-2507-4bit MODELS_DIR=~/models_mlx

# List downloaded models
make models-local
```

**Implementation:**

```bash
# scripts/download_model.sh
# 1. huggingface-cli download $MODEL --local-dir $MODELS_DIR/$MODEL_NAME
# 2. For each node in hostfile (except rank 0):
#      ssh $node "mkdir -p $MODELS_DIR"
#      rsync -avz --progress $LOCAL_PATH/ $node:$MODELS_DIR/$MODEL_NAME/
# 3. Verify all nodes have the model (checksum on config.json)
```

### 6.2 Model Registry

A simple JSON file tracking downloaded models:

```json
// ~/.mlx-jaccl-cluster/models.json
{
  "models": [
    {
      "id": "Qwen3-4B-Instruct-2507-4bit",
      "source": "mlx-community/Qwen3-4B-Instruct-2507-4bit",
      "path": "/Users/omar/models_mlx/Qwen3-4B-Instruct-2507-4bit",
      "size_gb": 2.4,
      "downloaded_at": "2025-07-14T10:23:00Z",
      "synced_nodes": ["mac.home", "mac2"],
      "quantization": "4-bit"
    }
  ]
}
```

### 6.3 Dashboard Model Manager Panel

```
â”Œâ”€â”€â”€ Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  â— Active: Qwen3-4B-Instruct-2507-4bit (4-bit, 2.4 GB)               â”‚
â”‚                                                                         â”‚
â”‚  Downloaded:                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Model                          â”‚ Size â”‚ Quant â”‚ Synced           â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Qwen3-4B-Instruct-2507-4bit   â”‚ 2.4G â”‚ 4-bit â”‚ âœ… 2/2 nodes    â”‚  â”‚
â”‚  â”‚ Llama-3.1-8B-Instruct-4bit    â”‚ 4.5G â”‚ 4-bit â”‚ âœ… 2/2 nodes    â”‚  â”‚
â”‚  â”‚ Mistral-7B-v0.3-4bit          â”‚ 3.8G â”‚ 4-bit â”‚ âš ï¸ 1/2 nodes    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  [Download New Model]  input: ______________________________  [Go]     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.4 Download Progress via SSE

When a download is in progress, the dashboard shows a progress bar:

```
Downloading: mlx-community/Qwen3-8B-Instruct-4bit
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚ 62% â€” 1.5 / 2.4 GB â€” 45 MB/s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Syncing to mac2... waiting
```

**Implementation:** A background asyncio task wraps `huggingface-cli download` subprocess, parses progress from stderr, pushes updates via a new SSE event type.

---

## 7. Phase 4 â€” Tool Support & Structured Output

> **Goal:** Support OpenAI function calling and JSON mode so agents and LangChain work.
> **Effort:** ~4â€“5 days. **Impact:** Unlocks the agent/tool ecosystem.

### 7.1 What Tool Calls Look Like

OpenAI tool calling request:

```json
{
  "model": "Qwen3-4B",
  "messages": [
    {"role": "user", "content": "What's the weather in Paris?"}
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get current weather for a city",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {"type": "string"}
          },
          "required": ["city"]
        }
      }
    }
  ]
}
```

Expected response:

```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": null,
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"city\": \"Paris\"}"
        }
      }]
    },
    "finish_reason": "tool_calls"
  }]
}
```

### 7.2 Implementation Plan

**Step 1: Prompt formatting.** Convert tools + messages into a prompt the model understands. Most instruct models (Qwen, Llama, Mistral) have specific chat templates for tool use that `tokenizer.apply_chat_template` already handles when `tools=` is passed.

```python
# In _build_chat_prompt(), add tools support:
def _build_chat_prompt(messages, tools=None):
    msgs = [{"role": m.role, "content": m.content} for m in messages]
    kwargs = {"tokenize": False, "add_generation_prompt": True}
    if tools:
        kwargs["tools"] = tools
    return _tok.apply_chat_template(msgs, **kwargs)
```

**Step 2: Response parsing.** After generation, detect if the output contains a tool call (model-specific format) and parse it into the OpenAI tool_calls structure.

```python
# Tool call detection (model-dependent patterns)
# Qwen3:   <tool_call>{"name": "...", "arguments": {...}}</tool_call>
# Llama:   <|python_tag|>{"name": "...", "parameters": {...}}
# Mistral: [TOOL_CALLS][{"name": "...", "arguments": {...}}]

def parse_tool_calls(text: str, model_family: str) -> list[dict] | None:
    """Extract tool calls from model output. Returns None if no tool call detected."""
    ...
```

**Step 3: Schema updates.** Extend `ChatCompletionsReq` and response schemas:

```python
class Tool(BaseModel):
    type: str = "function"
    function: dict  # {name, description, parameters}

class ChatCompletionsReq(BaseModel):
    model: Optional[str] = None
    messages: list[ChatMessage]
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    tools: Optional[list[Tool]] = None
    tool_choice: Optional[str] = None  # "auto", "none", or {"type":"function","function":{"name":"..."}}
    response_format: Optional[dict] = None  # {"type": "json_object"}
```

**Step 4: Streaming tool calls.** SSE chunks must include `tool_calls` delta objects per the OpenAI spec.

### 7.3 Structured Output / JSON Mode

When `response_format: {"type": "json_object"}` is set:

1. Append `"Respond with valid JSON."` to the system prompt
2. After generation, validate that the output is valid JSON
3. If not valid, retry once with a stronger prompt
4. Return `finish_reason: "stop"` only if valid JSON

### 7.4 Sampling Parameters

Currently missing â€” easy wins:

| Parameter | Default | Notes |
|---|---|---|
| `temperature` | 1.0 | Pass to `generate()` / `stream_generate()` |
| `top_p` | 1.0 | Nucleus sampling |
| `top_k` | -1 | Top-k sampling (-1 = disabled) |
| `repetition_penalty` | 1.0 | Penalize repeated tokens |
| `stop` | `[]` | Stop sequences â€” check after each token |

All of these are already supported by `mlx_lm.generate()` â€” we just need to plumb them through from the HTTP request.

---

## 8. Phase 5 â€” API Parity & Ecosystem

> **Goal:** Drop-in replacement for more clients.
> **Effort:** ~3â€“4 days. **Impact:** Works with Ollama clients, LangChain, etc.

### 8.1 Ollama API Compatibility

Many tools (Open WebUI, Continue.dev, etc.) speak Ollama's API:

| Endpoint | Method | Description |
|---|---|---|
| `/api/generate` | POST | Text generation (Ollama format) |
| `/api/chat` | POST | Chat (Ollama format) |
| `/api/tags` | GET | List models |
| `/api/show` | POST | Model info |
| `/api/ps` | GET | Running models |

**Implementation:** Thin adapter layer that translates Ollama requests â†’ our internal format â†’ Ollama responses. ~200 lines.

### 8.2 Additional OpenAI Endpoints

| Endpoint | Effort | Notes |
|---|---|---|
| `/v1/embeddings` | Medium | Requires an embedding model or adapter |
| `/v1/models/{id}` | Trivial | Return model details |
| `/v1/chat/completions` with `n > 1` | Medium | Multiple completions per request |

### 8.3 Client SDK Compatibility Testing

Verify against:

- [ ] OpenAI Python SDK (`openai.ChatCompletion.create()`)
- [ ] OpenAI Node SDK
- [ ] LangChain (`ChatOpenAI`)
- [ ] LlamaIndex
- [ ] Continue.dev (VS Code)
- [ ] Open WebUI
- [ ] Cursor / Cody (via OpenAI-compatible endpoint)
- [ ] `curl` (already tested)

---

## 9. Phase 6 â€” Advanced Inference

> **Goal:** Performance and capability improvements.
> **Effort:** ~1â€“2 weeks per feature. **Impact:** Competitive with production inference servers.

### 9.1 KV Prefix Cache

**What it does:** Caches the key-value tensors for shared prompt prefixes. If 10 users ask questions with the same system prompt, the KV cache for that prefix is computed once.

**Impact:** Dramatic latency reduction for chat applications with long system prompts.

**Complexity:** High â€” requires modifying how we call `generate()` and managing a cache eviction policy. May need `mlx_lm` updates.

### 9.2 Continuous Batching

**What it does:** Instead of processing one request at a time (current behavior), interleave tokens from multiple requests.

**Impact:** Higher throughput under concurrent load. Currently our queue processes requests serially.

**Complexity:** Very high â€” requires rewriting the generation loop. The control-plane protocol would need significant changes since all ranks must agree on batch composition.

### 9.3 Speculative Decoding

**What it does:** Use a small draft model to propose tokens, then verify with the large model in parallel.

**Impact:** 2â€“3Ã— speedup for large models.

**Complexity:** High â€” requires loading two models and coordinating draft/verify cycles across ranks.

---

## 10. Non-Goals

Things we deliberately choose NOT to implement:

| Feature | Reason |
|---|---|
| **Auto-discovery** | Explicit hostfile is simpler, more reliable, and correct for 1â€“4 node clusters |
| **Leader election** | Adds complexity; rank 0 is always the coordinator; 2-node clusters don't need it |
| **Image generation** | Different workload; use a dedicated tool (mflux, ComfyUI) |
| **SvelteKit dashboard** | Requires Node.js build toolchain; HTMX+SSE is sufficient and zero-build |
| **Custom MLX fork** | We use stock PyPI `mlx`; this is a core advantage |
| **Rust / Swift components** | Pure Python + Bash; zero build toolchain is a feature |
| **N > 4 node scaling** | JACCL requires fully connected TB mesh; 4 nodes = 6 cables, already impractical |
| **Multi-tenant isolation** | Single-user inference server; auth/isolation adds complexity for no benefit |

---

## 11. Architecture Decisions

### AD-01: Keep HTMX+SSE for Dashboard v2

**Context:** exo uses SvelteKit + D3.js for a richer dashboard.

**Decision:** Stay with HTMX + SSE + inline HTML/CSS/JS.

**Rationale:**
- Zero build step is a core project value
- HTMX can handle all planned features (memory bars, topology, tables)
- SVG can replace D3.js for the topology graph
- SSE is already working and battle-tested
- Adding Node.js + npm + Svelte contradicts our "zero toolchain" promise

### AD-02: Use Control-Plane for Metrics Transport

**Context:** Workers need to report memory/health to rank 0 for the dashboard.

**Decision:** Extend the existing TCP control-plane protocol with new message types (`metrics`, `heartbeat`).

**Rationale:**
- No new connections or ports needed
- Protocol is already framed JSON, easy to extend
- Workers already have an open socket to rank 0
- Alternative (HTTP from workers) would require each worker to run a server

### AD-03: Model Downloads via CLI, Not HTTP

**Context:** exo has a built-in `DownloadCoordinator` that downloads models via HTTP from a leader.

**Decision:** Use `huggingface-cli download` + `rsync` wrapped in a script.

**Rationale:**
- HuggingFace CLI handles auth, resume, checksums, LFS
- rsync is battle-tested for large file sync
- Building a download coordinator is high effort, low marginal value for 2 nodes
- Script approach is debuggable and composable

### AD-04: Tool Call Parsing is Model-Specific

**Context:** Different model families use different formats for tool calls.

**Decision:** Implement a pluggable parser with model-family detection.

**Rationale:**
- Qwen, Llama, and Mistral all use different tool call formats
- A single regex won't work
- Auto-detect model family from `config.json` or tokenizer config
- Start with Qwen3 (our primary model), add others incrementally

---

## 12. Dashboard v2 â€” Wireframe

### Mobile / Narrow Viewport

For access from phones or narrow windows, the grid collapses to single column:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš¡ mlx-jaccl-cluster    â”‚
â”‚ Qwen3-4B â”‚ â— Online     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cluster Overview         â”‚
â”‚ 2/2 nodes â”‚ 1,423 reqs  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Performance              â”‚
â”‚ 62.3 tok/s â”‚ 4.12s lat  â”‚
â”‚ â–â–ƒâ–…â–‡â–…â–ƒâ–…â–‡â–…â–ƒ (sparkline) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RDMA: 7.92 GB/s â— UP    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memory                   â”‚
â”‚ R0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 78%       â”‚
â”‚ R1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 77%       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Queue: 1/8 â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chat UI                  â”‚
â”‚ [message input]  [Send]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Navigation

No SPA routing needed. Single page with anchor links and collapsible sections:

```
[Overview] [Memory] [RDMA] [Requests] [Models] [Chat]
```

Each section is an HTMX fragment that auto-updates via SSE. No full page reloads.

---

## 13. Implementation Notes

### File Changes by Phase

**Phase 1 â€” Observability:**

| File | Change |
|---|---|
| `server/openai_cluster_server.py` | Add heartbeat protocol, memory probe, worker health tracking |
| `server/dashboard.py` | Add memory panel, RDMA panel, request history, error log |
| `server/rdma_probe.py` | **New** â€” Background RDMA health probe |
| `server/prometheus.py` | **New** â€” `/metrics` endpoint |
| `Makefile` | Add `status`, `logs`, `monitor` targets |
| `scripts/cluster_status.sh` | **New** â€” Full cluster status script |

**Phase 2 â€” Dashboard v2:**

| File | Change |
|---|---|
| `server/dashboard.py` | Major rewrite â€” new layout, panels, SSE v2 schema |
| `server/openai_cluster_server.py` | Pass new data sources to dashboard |

**Phase 3 â€” Model Management:**

| File | Change |
|---|---|
| `scripts/download_model.sh` | **New** â€” Download + sync script |
| `Makefile` | Add `download`, `models-local`, `models-sync` targets |
| `server/openai_cluster_server.py` | Add `/models/download` endpoint (optional) |
| `server/dashboard.py` | Add models panel |

**Phase 4 â€” Tool Support:**

| File | Change |
|---|---|
| `server/openai_cluster_server.py` | Tool calls in request/response, sampling params, stop sequences |
| `server/tool_parser.py` | **New** â€” Model-specific tool call parser |
| `server/schemas.py` | **New** â€” Extracted Pydantic models with tools support |

**Phase 5 â€” API Parity:**

| File | Change |
|---|---|
| `server/ollama_compat.py` | **New** â€” Ollama API adapter |
| `server/openai_cluster_server.py` | Mount Ollama routes |

### Control-Plane Protocol v2

Current message types:

```
â†’ workerâ†’rank0:  {"type": "hello", "rank": N}
â†’ rank0â†’worker:  {"type": "task", "prompt": "...", "max_tokens": N}
â†’ workerâ†’rank0:  {"type": "done", "rank": N}
```

v2 additions:

```
â†’ workerâ†’rank0:  {"type": "heartbeat", "rank": N, "memory": {...}, "timestamp": T}
â†’ rank0â†’worker:  {"type": "config", "probe_interval_s": 5}
â†’ workerâ†’rank0:  {"type": "metrics", "rank": N, "memory": {...}}
â†’ rank0â†’worker:  {"type": "shutdown"}  (graceful stop)
```

---

## 14. Dependency Budget

We're strict about dependencies. Every new package must justify itself.

### Current Dependencies (8 packages)

```
mlx >= 0.30.4
mlx-lm >= 0.30.5
fastapi >= 0.110.0
uvicorn[standard] >= 0.29.0
pydantic >= 2.0
transformers >= 4.50.0
tokenizers
mistral_common
huggingface_hub
```

### Planned Additions

| Package | Phase | Justification | Alternative Considered |
|---|---|---|---|
| `psutil` | Phase 1 | Cross-platform memory/CPU stats | `/proc` parsing (Linux only, we're macOS) |
| â€” | â€” | â€” | â€” |

That's it. **One new dependency** across all phases. Everything else is built with the standard library, MLX APIs, or inline code.

- Prometheus export: hand-written text format (no `prometheus_client`)
- Topology SVG: inline SVG (no D3.js)
- RDMA probe: `mx.distributed.all_sum` (already available)
- Download: `huggingface-cli` subprocess (already installed)

---

## 15. Success Criteria

### Phase 1 (Observability) â€” Done When:

- [ ] Dashboard shows live RAM usage per node (auto-refreshing)
- [ ] Dashboard shows live RDMA bandwidth and latency (from actual probes)
- [ ] Worker disconnect is detected within 15 seconds and shown on dashboard
- [ ] `make status` prints a complete cluster snapshot in the terminal
- [ ] `/metrics` returns valid Prometheus exposition format
- [ ] Request history is visible in dashboard (last 50 requests)

### Phase 2 (Dashboard v2) â€” Done When:

- [ ] Dashboard has all panels from the wireframe
- [ ] SVG topology graph shows nodes and RDMA links with live status
- [ ] Dashboard works on mobile (responsive layout)
- [ ] SSE pushes v2 schema with memory + RDMA + node data
- [ ] Zero build step maintained (no npm, no bundler)

### Phase 3 (Model Management) â€” Done When:

- [ ] `make download MODEL=...` downloads and syncs to all nodes
- [ ] `make models-local` lists all downloaded models with sizes
- [ ] Dashboard shows downloaded models and active model
- [ ] Download progress is visible (terminal and/or dashboard)

### Phase 4 (Tool Support) â€” Done When:

- [ ] OpenAI SDK `tools=` parameter works with Qwen3 models
- [ ] `response_format: {"type": "json_object"}` works
- [ ] `temperature`, `top_p`, `stop` parameters are plumbed through
- [ ] Streaming tool calls work per OpenAI spec
- [ ] LangChain `ChatOpenAI` with tools works against our server

### Phase 5 (API Parity) â€” Done When:

- [ ] Open WebUI connects via Ollama API and works for chat
- [ ] Continue.dev (VS Code) works with our server as OpenAI backend
- [ ] All Ollama core endpoints return valid responses

---

## Timeline (Estimated)

```
Week 1-2:   Phase 1 â€” Observability & Monitoring
Week 3-4:   Phase 2 â€” Dashboard v2
Week 5:     Phase 3 â€” Model Management
Week 6-7:   Phase 4 â€” Tool Support
Week 8:     Phase 5 â€” API Parity
Ongoing:    Phase 6 â€” Advanced Inference (opportunistic)
```

---

## Appendix: exo Features We're Deliberately Skipping

For completeness, these exo features are **not** on our roadmap and why:

| exo Feature | Lines of Code | Why We Skip It |
|---|---|---|
| libp2p auto-discovery | ~2,000 | Explicit hostfile is more reliable for 1â€“4 nodes |
| Raft leader election | ~1,500 | 2-node cluster doesn't need it |
| `DownloadCoordinator` | ~800 | `huggingface-cli` + `rsync` is simpler and more robust |
| Topology-aware shard placement | ~600 | 2-node = trivial placement |
| Flux image generation | ~400 | Use dedicated tools (mflux, ComfyUI) |
| Swift `SystemProfiler` integration | ~300 | `mlx.device_info()` gives us what we need |
| Nix flake | ~200 | `uv` + `pyproject.toml` is sufficient |
| Ring-buffer P2P topology | ~500 | JACCL handles topology via hostfile |
| Custom MLX fork maintenance | ongoing | Using stock PyPI mlx is a core advantage |

**Total lines we avoid maintaining: ~6,300+**

---

*This roadmap is a living document. Update it as phases are completed or priorities shift.*