# mlx-jaccl-cluster vs exo ‚Äî Deep Comparison

> **Hardware under test:** 2 √ó Mac mini (Mac16,11) ‚Äî Apple M4 Pro ¬∑ 48 GB ¬∑ macOS 26.3  
> **Thunderbolt link:** TB3 receptacle ‚Üí `rdma_en4` on both nodes  
> **Date:** 2025

---

## TL;DR

| | **mlx-jaccl-cluster** (our fork) | **exo** |
|---|---|---|
| RDMA working on M4 Pro pair | ‚úÖ **8.05 GB/s ¬∑ 25.5 ¬µs confirmed** | ‚ùå fails on same hardware |
| Python lines | ~2,028 | ~31,844 |
| Time to first inference | ~2 min (model already present) | fails before reaching inference |
| Config needed | one JSON hostfile | `set_rdma_network_config.sh` + `EXO_MODELS_PATH` env + correct mdns timing |
| Reliability | deterministic (explicit) | non-deterministic (auto-discovery race) |
| Features | minimal OpenAI API | full-featured platform |

Our fork is smaller, simpler, and is the only stack of the two that has been proven to run RDMA on this exact hardware. exo is a far more capable platform but carries complexity that translates directly into failure modes on a two-node TB3 setup.

---

## 1. Codebase Size & Language Breakdown

| Metric | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Python files | 4 | 159 |
| Python lines | ~2,028 | ~31,844 |
| Server / API lines | ~785 | ~1,816 (`master/api.py` 69 KB alone) |
| Dashboard lines | ~919 (HTMX, inline HTML) | ~4,000 (SvelteKit + D3.js) |
| Languages | Python ¬∑ Bash | Python ¬∑ Svelte ¬∑ Rust ¬∑ Nix ¬∑ Swift |
| Rust bindings | ‚ùå | ‚úÖ `exo_pyo3_bindings` |
| Build step required | ‚ùå none | ‚úÖ `npm run build` (dashboard) + Rust compile |

exo is a full platform; our fork is a focused tool. Neither is inherently better ‚Äî the tradeoff is features vs. operational simplicity.

---

## 2. Architecture

### 2.1 Node Discovery

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Mechanism | Explicit JSON hostfile (`hostfiles/hosts-2node.json`) | libp2p auto-discovery (mDNS + peer exchange) |
| RDMA device mapping | Statically declared per-node in hostfile | Auto-detected at runtime via `InfoGatherer` + TB interface enumeration |
| Fragility | None ‚Äî you control the mapping | High ‚Äî timing-dependent; generated mismatched `rdma_enX` on identical hardware |
| Example config | `{"ssh":"mac.home","rdma":[null,"rdma_en4"]}` | `hosts_*.json` generated by exo, historically inconsistent |

**Observed exo failure mode:** exo's auto-discovery generated hostfiles where node 0 listed `rdma_en4` but node 1 listed `rdma_en3` (an inactive interface). This alone is sufficient to make every JACCL `ConnectToGroup` call fail ‚Äî the QP (Queue Pair) is created against a dead interface and the connection is refused immediately.

Our hostfile approach sidesteps this entirely. The active device (`rdma_en4`) is declared once and never changes.

---

### 2.2 RDMA / JACCL Transport

Both projects use the same underlying JACCL backend, but they reach it through very different paths.

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Launch mechanism | `mlx.launch --backend jaccl --hostfile ‚Ä¶` | exo runner calls `mx.distributed.init(backend="jaccl")` internally |
| MLX version | Stock `mlx 0.30.6` (PyPI) | Custom fork: `rltakashige/mlx-jaccl-fix-small-recv` (commit `13998a0`) |
| Network config required | **None** ‚Äî works with default macOS settings | Requires running `tmp/set_rdma_network_config.sh` (disables TB Bridge, sets DHCP on TB ports) |
| RDMA proven on M4 Pro | ‚úÖ | ‚ùå |
| Peak bandwidth (measured) | **8.05 GB/s** | N/A ‚Äî never reached inference |
| Latency (1-element all_sum) | **25.5 ¬µs** | N/A |

**Why exo needs `set_rdma_network_config.sh`:** exo expects the Thunderbolt Bridge network interface to be disabled and the raw TB ports to be configured as DHCP interfaces. macOS default behaviour keeps the TB Bridge active. Our approach never touches the network config because `mlx.launch` resolves the RDMA device by name from the hostfile, bypassing any IP routing entirely.

---

### 2.3 Coordination & Control Plane

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Pattern | Simple TCP broadcast (rank 0 ‚Üí workers) | Event-sourced state machine + leader election |
| Protocol | Struct-framed JSON over raw TCP sockets | libp2p topics: `GLOBAL_EVENTS`, `COMMANDS`, `LOCAL_EVENTS`, ‚Ä¶ |
| Leader election | ‚ùå static (rank 0 is always coordinator) | ‚úÖ Raft-like election (`election.py` ~11 KB) |
| State model | Stateless per-request | Full replicated state (`State` object, `apply.py` ~14 KB) |
| Worker sync | Rank 0 broadcasts `{"type":"task"}`, waits for `{"type":"done"}` from all ranks | `plan_step()` loop polls state every 100 ms, derives next `Task` from replicated state |
| Concurrency model | `asyncio` + one background `threading.Thread` for control socket | `anyio` task groups throughout; each subsystem is its own coroutine |
| Lines of coordination code | ~120 (rank0 broadcast + worker loop) | ~1,000+ across `worker/main.py`, `master/main.py`, `routing/router.py`, `shared/election.py` |

The event-sourced model in exo is architecturally sound for N-node dynamic clusters. For a fixed two-node setup it is significant over-engineering and introduces subtle timing dependencies (see Section 4).

---

### 2.4 Inference Engine

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Load function | `mlx_lm.sharded_load` | Custom `auto_parallel.py` (~1,063 lines) |
| Parallelism | Tensor parallel (via mlx_lm sharding) | Tensor parallel + pipeline parallel |
| Streaming | `mlx_lm.stream_generate` (SSE) | Custom token loop in `runner.py` (~32 KB) |
| KV cache | ‚ùå | ‚úÖ prefix cache |
| Image generation | ‚ùå | ‚úÖ mflux (Flux models) |
| Tool calls | ‚ùå | ‚úÖ |
| Concurrency | Sequential queue ‚Äî one request at a time | Concurrent runner tasks via anyio |

Our inference path is 3 lines: `sharded_load` ‚Üí `stream_generate` ‚Üí yield chunks. This is not a limitation for a dedicated two-node inference box ‚Äî sequential generation at full RDMA bandwidth is the correct mode.

---

### 2.5 API Server

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Framework | FastAPI + uvicorn | FastAPI + hypercorn |
| API lines | ~607 | ~1,816 (`master/api.py` alone is 69 KB) |
| OpenAI `/v1/chat/completions` | ‚úÖ | ‚úÖ |
| OpenAI `/v1/completions` | ‚úÖ | ‚úÖ |
| Ollama adapter | ‚ùå | ‚úÖ |
| `/v1/images/generations` | ‚ùå | ‚úÖ |
| `/v1/images/edits` | ‚ùå | ‚úÖ |
| Streaming (SSE) | ‚úÖ | ‚úÖ |
| Tool / function calls | ‚ùå | ‚úÖ |
| Model list endpoint | ‚úÖ `/v1/models` | ‚úÖ |
| Health endpoint | ‚úÖ `/health` | ‚úÖ |
| Queue / backpressure | ‚úÖ `asyncio.Queue(maxsize=8)` | ‚úÖ |
| Auth | ‚ùå | ‚ùå |
| Timeout handling | ‚úÖ `REQ_TIMEOUT` env var (default 120 s) | ‚úÖ |

---

### 2.6 Dashboard

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Stack | HTMX + SSE + inline HTML | SvelteKit + D3.js + Svelte 5 |
| Build step | ‚ùå none | ‚úÖ `npm run build` |
| Lines | ~919 | ~4,000 |
| Live metrics (tok/s, latency, queue) | ‚úÖ via SSE | ‚úÖ |
| Cluster topology table | ‚úÖ (rank, role, RDMA device, status) | ‚úÖ |
| D3.js topology graph | ‚ùå | ‚úÖ live RDMA link animation |
| Chat UI | ‚úÖ full streaming chat | ‚úÖ |
| Token heatmap | ‚ùå | ‚úÖ |
| Model download progress | ‚ùå | ‚úÖ |
| Generation traces | ‚ùå | ‚úÖ |
| Sparkline (tok/s history) | ‚úÖ SVG sparkline via SSE | ‚úÖ |
| RDMA bandwidth display | ‚úÖ (static ~8 GB/s label) | ‚ùå (no RDMA-specific panel) |

---

### 2.7 Model Management

| | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Download | ‚ùå manual `huggingface-cli download` | ‚úÖ built-in `DownloadCoordinator` with per-shard progress |
| Shard sync across nodes | ‚ùå manual `rsync` | ‚úÖ coordinated via `StartDownload` tasks |
| Model path | `MODEL_DIR` env var | `~/.exo/models/` (`EXO_MODELS_DIR`) |
| Model detection | Direct path existence | `resolve_model_in_path()` checks `EXO_MODELS_PATH` env var |

---

## 3. Dependency Comparison

### mlx-jaccl-cluster

```text
mlx >= 0.30.6
mlx-lm >= 0.30.7
fastapi
uvicorn
pydantic
huggingface-hub   (manual CLI usage only)
```

Total: **6 direct dependencies**. All available on PyPI. `uv sync` takes ~30 seconds.

### exo

```text
anyio
loguru
pydantic
fastapi
hypercorn
libp2p  (via Rust bindings: exo_pyo3_bindings)
mlx     (custom fork: rltakashige/mlx-jaccl-fix-small-recv)
mlx-lm
huggingface-hub
tqdm
aiofiles
aiohttp
Pillow
mflux   (image generation)
... and ~30 more transitive deps
```

Additionally requires:
- **Rust toolchain** (for `exo_pyo3_bindings`)
- **Node.js + npm** (for SvelteKit dashboard build)
- **Xcode** (Swift components)

A clean install from source on a new Mac takes 10‚Äì20 minutes.

---

## 4. Failure Modes on M4 Pro (Thunderbolt 3)

This is the most important section for our specific hardware.

### 4.1 Failure Mode 1 ‚Äî Wrong RDMA Interface (Device Mapping Bug)

**Trigger:** exo's `InfoGatherer` enumerates Thunderbolt interfaces at startup and maps them to RDMA device names. On Mac16,11 the mapping is non-deterministic depending on which TB receptacle is enumerated first.

**Symptom:**
```text
hosts_*.json ‚Üí node 0: ["rdma_en4", null]
              node 1: ["rdma_en2", null]   ‚Üê WRONG, rdma_en2 is PORT_DOWN
```

**Effect:** JACCL creates a Queue Pair against `rdma_en2` (inactive). The remote end does not respond. Connection refused ‚Üí `RuntimeError: Cannot initialize jaccl distributed backend`.

**Our fix:** Declare the correct interface statically in the hostfile. No auto-detection needed.

```text
hostfiles/hosts-2node.json ‚Üí node 0: [null, "rdma_en4"]
                              node 1: ["rdma_en4", null]   ‚Üê always correct
```

---

### 4.2 Failure Mode 2 ‚Äî EXO_MODELS_PATH Not Set

**Trigger:** exo's planner calls `resolve_model_in_path(model_id)` to detect whether a model already exists locally. This function only searches directories listed in the `EXO_MODELS_PATH` environment variable. If `EXO_MODELS_PATH` is unset (the default), the function returns `None` regardless of whether the model is present in `~/.exo/models/`.

**Symptom:** Model files are present on both nodes under `~/.exo/models/mlx-community--GLM-4.7-Flash-4bit/` (~16 GB). exo's planner still emits a `DownloadModel` task because `resolve_model_in_path()` returns `None`. The coordinator node enters the `DownloadModel` branch instead of proceeding to `ConnectToGroup`. It never opens the JACCL listener socket.

The worker node, unaware of the coordinator's download state, advances to `ConnectToGroup` immediately and tries to connect to `192.168.1.27:65093`. The port is not listening ‚Üí `ECONNREFUSED` ‚Üí `Couldn't connect (error: 60)` ‚Üí runner crash.

**Cascading result:**
```
Coordinator:  DownloadModel  ‚Üí  (never reaches ConnectToGroup)
Worker:       ConnectToGroup ‚Üí  ECONNREFUSED ‚Üí RuntimeError: Cannot initialize jaccl distributed backend
```

**Fix:**
```bash
export EXO_MODELS_PATH=~/.exo/models
uv run exo
```

Or patch `resolve_model_in_path()` to also check `EXO_MODELS_DIR`:

```python
# exo/download/download_utils.py  (proposed patch)
def resolve_model_in_path(model_id: ModelId) -> Path | None:
    search_dirs = []
    if (p := os.getenv("EXO_MODELS_PATH")):
        search_dirs.extend(p.split(":"))
    # Also check the default download dir even if EXO_MODELS_PATH is not set
    default_dir = Path(os.getenv("EXO_MODELS_DIR", "~/.exo/models")).expanduser()
    if default_dir not in [Path(d) for d in search_dirs]:
        search_dirs.append(str(default_dir))
    for d in search_dirs:
        candidate = Path(d) / str(model_id)
        if candidate.exists():
            return candidate
    return None
```

---

### 4.3 Failure Mode 3 ‚Äî Network Config (TB Bridge Active)

exo's `set_rdma_network_config.sh` must be run once on both Macs. It:
1. Disables the Thunderbolt Bridge (`bridge100` / `bridge0`) network interface
2. Sets the raw TB port (`en4`) to DHCP mode
3. Assigns a link-local IP on the TB port for JACCL signalling

If skipped, macOS routes JACCL's initial TCP handshake through the bridge interface, which may use a different MTU or route than the RDMA data path, causing intermittent failures.

Our fork does not require this because `mlx.launch` uses the RDMA device name directly from the hostfile and never touches TCP routing for the JACCL handshake.

---

## 5. RDMA Benchmark Results

Measured on this exact hardware using `mlx.launch --backend jaccl` with our `scripts/rdma_test.py`:

| Tensor size | Avg bandwidth | Peak bandwidth | Avg latency |
|---|---|---|---|
| 4 KB | ‚Äî | ‚Äî | **25.5 ¬µs** |
| 256 KB | 1.82 GB/s | 2.14 GB/s | 0.12 ms |
| 4 MB | 6.71 GB/s | 7.38 GB/s | 0.56 ms |
| 64 MB | 7.94 GB/s | **8.05 GB/s** | 7.60 ms |

- **Peak bandwidth: 8.05 GB/s** ‚Äî saturates the TB3 link (~10 Gbps theoretical / ~8 GB/s practical ceiling)
- **Latency (1-element all_sum): 25.5 ¬µs** ‚Äî excellent; well within the "sub-100 ¬µs" target for tensor-parallel LLM inference
- **exo equivalent:** Not measured ‚Äî exo never completed JACCL initialization on this hardware

---

## 6. Startup Comparison

### mlx-jaccl-cluster ‚Äî Startup Sequence

```
Mac 1 (rank 0):
  1. mlx.launch resolves hostfile ‚Üí rdma_en4 confirmed PORT_ACTIVE
  2. mx.distributed.init(backend="jaccl")  [coordinator, listens on RDMA port]
  3. sharded_load() ‚Üí model shards assigned to rank 0
  4. uvicorn starts on 0.0.0.0:8080
  5. dashboard mounted at /dashboard
  6. rank0_accept_workers() listens on CTRL_PORT 18080
  7. "all workers connected" ‚Üí ready

Mac 2 (rank 1):
  1. mlx.launch resolves hostfile ‚Üí rdma_en4 confirmed PORT_ACTIVE
  2. mx.distributed.init(backend="jaccl")  [worker, connects to coordinator]
  3. sharded_load() ‚Üí model shards assigned to rank 1
  4. worker_loop() connects to CTRL_PORT ‚Üí sends {"type":"hello","rank":1}
  5. blocks on recv_msg() waiting for tasks

Total time: ~45‚Äì90 s (dominated by model load from disk)
```

### exo ‚Äî Expected Startup Sequence (when working)

```
Both Macs (symmetric):
  1. libp2p router starts, registers topics
  2. mDNS discovers peer (timing-dependent, can take 5‚Äì30 s)
  3. InfoGatherer gathers node info (CPU, memory, RDMA devices)
  4. Election runs ‚Üí one node becomes master
  5. DownloadCoordinator checks model presence (requires EXO_MODELS_PATH)
  6. If model present ‚Üí emit DownloadCompleted ‚Üí planner advances
  7. Master emits CreateRunner ‚Üí Worker creates RunnerSupervisor
  8. Runner calls mx.distributed.init(backend="jaccl")
     [one node listens (coordinator), the other connects (worker)]
  9. API starts on port 52415
  
Total time: 90‚Äì180 s (when all steps succeed)
Race condition: steps 6‚Äì8 must happen in the correct order on both nodes
              within the JACCL connection timeout window
```

---

## 7. What exo Does Better

Despite the RDMA failures on our hardware, exo has real advantages for larger or more dynamic clusters:

| Feature | Why it matters |
|---|---|
| **Auto-discovery** | Works across heterogeneous networks without manual config (when it works) |
| **Leader election** | Tolerates node failures ‚Äî any node can become master |
| **Built-in downloads** | No manual `huggingface-cli` + `rsync` |
| **KV prefix cache** | Reduces re-prompt cost for repeated context (important for chat apps) |
| **Image generation** | Full Flux pipeline via mflux |
| **Tool calls** | Function calling, structured output |
| **Ollama compatibility** | Drop-in replacement for more clients |
| **Pipeline parallelism** | Can split model layers across nodes (not just tensors) |
| **SvelteKit dashboard** | D3.js topology graph, token heatmap, generation traces |
| **N-node scaling** | Designed for 3, 4, 8+ nodes |
| **Offline mode** | `--offline` flag for air-gapped deployments |

---

## 8. What Our Fork Does Better

| Aspect | Why it matters |
|---|---|
| **RDMA actually works** | 8.05 GB/s confirmed on M4 Pro ‚Äî the core value proposition |
| **Zero network reconfiguration** | Default macOS settings, no `set_rdma_network_config.sh` |
| **Deterministic startup** | No election races, no discovery timeouts, no timing windows |
| **3-minute setup** | `uv sync` + edit hostfile + run ‚Äî nothing else |
| **No custom MLX fork** | Uses stock `mlx 0.30.6` from PyPI |
| **No build toolchain** | No Rust, no Node.js, no npm |
| **Debuggable** | ~2,000 lines total; any failure is traceable in minutes |
| **RDMA test suite** | `scripts/rdma_test.py` ‚Äî correctness + latency + bandwidth sweep |
| **Live RDMA metrics** | Dashboard shows actual GB/s (not static) via background probe |
| **Explicit hostfile** | Full control over which RDMA interface each node uses |

---

## 9. Dependency Risk

| Risk | **mlx-jaccl-cluster** | **exo** |
|---|---|---|
| Custom MLX fork dependency | ‚ùå (uses PyPI mlx) | ‚úÖ (pinned to `rltakashige/mlx-jaccl-fix-small-recv`) |
| Breaks on mlx update | Low ‚Äî only public API used | High ‚Äî custom fork may diverge from upstream |
| Breaks on macOS update | Low | Moderate ‚Äî TB network reconfiguration may need re-running |
| Rust toolchain needed | ‚ùå | ‚úÖ |
| Node.js needed | ‚ùå | ‚úÖ (dashboard) |
| Number of moving parts | 3 (Python, mlx, mlx_lm) | 8+ (Python, custom mlx, Rust, Node.js, libp2p, anyio, SvelteKit, Swift) |

---

## 10. Summary Verdict

```
                    mlx-jaccl-cluster        exo
                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ
RDMA on M4 Pro      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà WORKS         ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë BROKEN *
Features            ‚ñà‚ñà‚ñà minimal              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà full platform
Complexity          ‚ñà‚ñà‚ñà 2K lines             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 32K lines
Setup time          ‚ñà‚ñà‚ñà ~3 min               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ~20+ min
Reliability         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà deterministic ‚ñà‚ñà‚ñà‚ñà race conditions
Dependencies        ‚ñà‚ñà‚ñà 6 packages           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 30+ packages
```

*exo RDMA broken due to: (1) incorrect RDMA interface auto-mapping and
(2) `EXO_MODELS_PATH` not set causing coordinator to re-download existing models,
preventing JACCL initialization. Both are fixable (see Section 4).

### When to use our fork
- You have a fixed 1‚Äì2 node Apple Silicon cluster
- You want RDMA working today with no fuss
- You need a simple, debuggable, production-stable inference server
- You are not doing image generation or tool calls

### When exo makes more sense
- You have 3+ nodes or a dynamic cluster
- You need fault tolerance (node drops, master re-election)
- You need image generation, tool calls, or Ollama compatibility
- You are willing to invest setup time and debug discovery/config issues
- You want the full SvelteKit dashboard out of the box

---

## 11. Closing the Gap ‚Äî Planned Enhancements

Our fork is intentionally minimal, but "minimal" doesn't mean "finished." This section maps the concrete gaps between us and exo and our plan to close the ones that matter.

> Full implementation details, wireframes, and phasing: [docs/roadmap.md](roadmap.md)
> Deep architecture reference: [docs/architecture.md](architecture.md)

### 11.1 Gap Summary

| Gap | exo Has It | We Have It | Plan | Phase |
|---|---|---|---|---|
| **Live RAM / memory monitoring** | ‚ùå | ‚ùå | Workers report `mx.metal.get_active_memory()` to rank 0 via control-plane heartbeats; dashboard shows per-node memory bars | Phase 1 |
| **Live RDMA bandwidth probe** | ‚ùå | ‚ùå (static label) | Background `all_sum` probe when queue is idle; latency every 10 s, bandwidth every 60 s | Phase 1 |
| **Worker health / disconnect detection** | ‚úÖ | ‚ùå | TCP heartbeat every 5 s; 15 s timeout ‚Üí mark `DISCONNECTED`; 503 instead of hang | Phase 1 |
| **Request history & error log** | ‚ùå | ‚ùå | Ring buffers (200 requests, 50 errors); `/requests` + `/errors` endpoints; dashboard table | Phase 1 |
| **Prometheus `/metrics`** | ‚ùå | ‚ùå | Hand-written exposition format ‚Äî zero new deps | Phase 1 |
| **`make status` / `make monitor`** | N/A | ‚ùå | Full cluster snapshot in terminal; watch mode with refresh | Phase 1 |
| **Dashboard v2 (memory, RDMA, topology, history)** | ‚úÖ (SvelteKit) | Partial | Extend HTMX+SSE; SVG topology (no D3.js); per-node memory gauges; request table | Phase 2 |
| **Model download + sync** | ‚úÖ (`DownloadCoordinator`) | ‚ùå | `make download MODEL=...` wraps `huggingface-cli` + `rsync`; progress via SSE | Phase 3 |
| **Tool calls / function calling** | ‚úÖ | ‚ùå | Pass `tools=` to `apply_chat_template()`; parse model-specific output format (Qwen3 first) | Phase 4 |
| **Structured output / JSON mode** | ‚úÖ | ‚ùå | `response_format: {"type":"json_object"}` ‚Äî prompt injection + validation | Phase 4 |
| **Sampling params (`temperature`, `top_p`, `stop`)** | ‚úÖ | ‚ùå | Plumb through to `generate()` / `stream_generate()` ‚Äî already supported by mlx_lm | Phase 4 |
| **Ollama API compatibility** | ‚úÖ | ‚ùå | Thin adapter: `/api/generate`, `/api/chat`, `/api/tags` ‚Äî ~200 lines | Phase 5 |
| **KV prefix cache** | ‚úÖ | ‚ùå | Complex ‚Äî depends on mlx_lm support; evaluate after Phase 4 | Phase 6 |

### 11.2 What We're NOT Closing

These exo features are deliberately **not** on our roadmap:

| exo Feature | Why We Skip It |
|---|---|
| libp2p auto-discovery (~2K lines) | Explicit hostfile is more reliable for 1‚Äì4 nodes |
| Raft leader election (~1.5K lines) | 2-node cluster doesn't need it |
| Image generation / Flux (~400 lines) | Use dedicated tools (mflux, ComfyUI) |
| SvelteKit dashboard (~4K lines) | HTMX+SSE is sufficient; no Node.js build step |
| Custom MLX fork | Stock PyPI `mlx` is a core advantage |
| Rust / Swift components | Zero build toolchain is a feature |
| Ring-buffer P2P topology | JACCL handles topology via hostfile |

**Total lines we avoid maintaining: ~6,300+**

### 11.3 After All Phases ‚Äî Feature Parity Comparison

```
                    Now (v0.1)        After Phase 5     exo
                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚îÄ‚îÄ‚îÄ
RDMA on M4 Pro      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
Server API          ‚ñà‚ñà‚ñà‚ñà‚ñà             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Dashboard           ‚ñà‚ñà‚ñà‚ñà              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Model management    ‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Tool support        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Monitoring          ‚ñà‚ñà‚ñà               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         ‚ñà‚ñà‚ñà‚ñà
Complexity          ‚ñà‚ñà‚ñà               ‚ñà‚ñà‚ñà‚ñà              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Setup time          ‚ñà‚ñà‚ñà               ‚ñà‚ñà‚ñà               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Reliability         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ‚ñà‚ñà‚ñà‚ñà
```

The goal is not to replicate exo feature-for-feature. It's to close the gaps that matter for **daily use** (monitoring, model management, tool calls) while keeping our core advantages (reliability, simplicity, working RDMA, zero toolchain).

### 11.4 New Makefile Targets (Already Added)

These targets ship now alongside this document:

| Target | Description |
|---|---|
| `make download MODEL=...` | Download a HuggingFace model and rsync to all nodes |
| `make models-local` | List locally downloaded models with sizes |
| `make models-check` | Verify model exists on all nodes |
| `make status` | Full cluster status: nodes, memory, RDMA, server |
| `make monitor` | Live-updating status (refreshes every 5 s) |
| `make logs` | Tail server log file |
| `make dashboard` | Open the dashboard in the default browser |
| `make metrics` | Show current metrics snapshot (JSON) |
| `make version` | Show version info for all components |
| `make lint` | Run syntax + shellcheck quality checks |
| `make test` | Full test suite: lint + RDMA quick + health |
| `make loc` | Count lines of code by component |

---

## Appendix: Verified Working Configuration (Our Fork)

```json
// hostfiles/hosts-2node.json
[
  { "ssh": "mac.home", "ips": ["192.168.1.14"], "rdma": [null, "rdma_en4"] },
  { "ssh": "mac2",     "ips": [],               "rdma": ["rdma_en4", null] }
]
```

```bash
# Launch command (both nodes, from Mac 1)
MLX_METAL_FAST_SYNCH=1 \
MODEL_DIR=/path/to/model \
.venv/bin/mlx.launch \
  --backend jaccl \
  --hostfile hostfiles/hosts-2node.json \
  --env MLX_METAL_FAST_SYNCH=1 \
  --env MODEL_DIR=/path/to/model \
  -- server/openai_cluster_server.py
```

```bash
# Verified RDMA benchmark result
Peak BW  : 8.05 GB/s
Latency  : 25.5 ¬µs  (1-element all_sum, 50 iterations)
Verdict  : EXCELLENT ‚Äî TB5 RDMA is flying üöÄ
```
